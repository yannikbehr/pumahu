{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "from sampling import (NestedSampling, Uniform,\n",
    "                      Callback, Normal, Constant,\n",
    "                      SamplingException)\n",
    "\n",
    "from filterpy.kalman import UnscentedKalmanFilter as UKF\n",
    "from filterpy.kalman import KalmanFilter as KF\n",
    "from filterpy.kalman import MerweScaledSigmaPoints\n",
    "from filterpy.common import Q_continuous_white_noise\n",
    "\n",
    "import progressbar\n",
    "\n",
    "from clemb.forward_model import forward_model, fullness, esol\n",
    "import clemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---------------------------\n",
    "1. [Data preparation](#Data preparation)\n",
    "2. [Inversion](#Inversion)\n",
    "3. [Prediction](#Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = '2009-09-25'\n",
    "tend = '2018-08-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_date(date):\n",
    "    \"\"\"\n",
    "    Function used for pandas group-by.\n",
    "    If there are several measurements in \n",
    "    one day, take the mean.\n",
    "    \"\"\"\n",
    "    ndt = pd.Timestamp(year=date.year,\n",
    "                       month=date.month,\n",
    "                       day=date.day)\n",
    "    return ndt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_mg(df, dt=1):\n",
    "    \"\"\"\n",
    "    Inter- and extrapolate Mg++ measurements using a\n",
    "    non-linear Kalman filter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def f_x(x, dt):\n",
    "        \"\"\"\n",
    "        Forward model exponential decay\n",
    "        \"\"\"\n",
    "        _k = x[1]/1e3\n",
    "        _dt = dt\n",
    "        _y = x[0]\n",
    "        if isinstance(dt, np.ndarray):\n",
    "            _dt = dt[0]\n",
    "        # 4th order Runge-Kutta\n",
    "        k0 = -_k * _y * _dt\n",
    "        k1 = -_k * (_y + 0.5 * k0) * _dt\n",
    "        k2 = -_k * (_y + 0.5 * k1) * _dt\n",
    "        k3 = -_k * (_y + k2) * _dt\n",
    "        _y_next = _y + 1./6.*(k0 + 2 * k1 + 2 * k2 + k3)\n",
    "        return np.array([_y_next, x[1]])\n",
    "\n",
    "    def h_x(x):\n",
    "        \"\"\"\n",
    "        Measurement function\n",
    "        \"\"\"\n",
    "        return [x[0]]\n",
    "    \n",
    "    dts = np.r_[0, np.cumsum(np.diff(df.index).astype(int)/(86400*1e9))]\n",
    "    dts = dts[:, np.newaxis]\n",
    "    ny = df['obs'].values\n",
    "    ny = np.where(np.isnan(ny), None, ny)\n",
    "    \n",
    "    points = MerweScaledSigmaPoints(n=2, alpha=.01, beta=2., kappa=1.)\n",
    "    kf = UKF(dim_x=2, dim_z=1, dt=dt, fx=f_x, hx=h_x, points=points)\n",
    "    kf.x = np.array([ny[0], .6])\n",
    "    kf.Q = Q_continuous_white_noise(2, dt=dt, spectral_density=1e-7)\n",
    "    kf.P = np.diag([100.**2, 3.**2])\n",
    "    kf.R = 50.**2\n",
    "    npoints = dts.size\n",
    "    means = np.zeros((npoints-1, 2))\n",
    "    covariances = np.zeros((npoints-1, 2, 2))\n",
    "    for i, z_n in enumerate(ny[1:]):\n",
    "        kf.predict()\n",
    "        kf.update(z_n)\n",
    "        means[i,:] = kf.x\n",
    "        covariances[i, :, :] = kf.P\n",
    "    Ms, P, K = kf.rts_smoother(means, covariances)\n",
    "    y_new = np.r_[ny[0], Ms[:,0]]\n",
    "    y_std = np.r_[50, np.sqrt(P[:, 0, 0])]\n",
    "    return pd.DataFrame({'obs': y_new,\n",
    "                         'obs_err': y_std,\n",
    "                         'orig': df['obs'].values},\n",
    "                        index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mg_data(tstart=None, tend=datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")):\n",
    "    # Get Mg++ concentration\n",
    "    url = \"https://fits.geonet.org.nz/observation?siteID=RU003&typeID=Mg-w\"\n",
    "    names = ['obs', 'obs_err']\n",
    "    mg_df = pd.read_csv(url, index_col=0, names=names, skiprows=1,\n",
    "                        parse_dates=True)\n",
    "    if tstart is not None:\n",
    "        tstart = max(mg_df.index.min(), pd.Timestamp(tstart))\n",
    "    else:\n",
    "        tstart = mg_df.index.min()\n",
    "    mg_df = mg_df.loc[(mg_df.index >= tstart) & (mg_df.index <= tend)]\n",
    "\n",
    "    mg_df = mg_df.groupby(common_date, axis=0).mean()\n",
    "    new_dates = pd.date_range(start=tstart, end=tend, freq='D')\n",
    "    mg_df = mg_df.reindex(index=new_dates)\n",
    "    # Find the first non-NaN entry\n",
    "    tstart_min = mg_df.loc[~mg_df['obs'].isnull()].index[0]\n",
    "    # Ensure the time series starts with a non-NaN value\n",
    "    mg_df = mg_df.loc[mg_df.index >= tstart_min]\n",
    "    return interpolate_mg(mg_df)\n",
    "\n",
    "df1 = get_mg_data(tstart=tstart, tend=tend)\n",
    "df1.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df1.index, df1['obs'], 'k--')\n",
    "plt.fill_between(df1.index, df1['obs']-3*df1['obs_err'],\n",
    "                  df1['obs']+3*df1['obs_err'], alpha=0.5)\n",
    "plt.plot(df1.index, df1['orig'], 'k+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_T(df, dt=1):\n",
    "    dts = np.r_[0, np.cumsum(np.diff(df.index).astype(int)/(86400*1e9))]\n",
    "    dts = dts[:, np.newaxis]\n",
    "    ny = df['t'].values\n",
    "    ny = np.where(np.isnan(ny), None, ny)\n",
    "    \n",
    "    kf = KF(dim_x=2, dim_z=1)\n",
    "    kf.F = np.array([[1, 1], [0, 1]])\n",
    "    kf.H = np.array([[1., 0]])\n",
    "    if ny[1] is not None:\n",
    "        dT0 = ny[1] - ny[0]\n",
    "    else:\n",
    "        dT0 = 0.\n",
    "    kf.x = np.array([ny[0], dT0])\n",
    "    kf.Q = Q_continuous_white_noise(2, dt=dt, spectral_density=3e-2)\n",
    "    kf.P *= 1e-5**2\n",
    "    kf.R = .5**2\n",
    "    means, covariances, _, _ =kf.batch_filter(ny[1:])\n",
    "    Ms, P, _, _ = kf.rts_smoother(means, covariances)\n",
    "    y_new = np.r_[ny[0], Ms[:,0]]\n",
    "    y_std = np.r_[.3, np.sqrt(P[:, 0, 0])]\n",
    "    return pd.DataFrame({'t': y_new,\n",
    "                         't_err': y_std,\n",
    "                         't_orig': df['t'].values},\n",
    "                         index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_T(tstart=None, tend=datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")):        \n",
    "    # Get temperature\n",
    "    # Temperature has been recorded by 3 different sensors so 3 individual\n",
    "    # requests have to be made\n",
    "    url = \"https://fits.geonet.org.nz/observation?siteID=RU001&typeID=t&methodID={}\"\n",
    "    names = ['t', 't_err']\n",
    "    tdf1 = pd.read_csv(url.format('therm'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    tdf2 = pd.read_csv(url.format('thermcoup'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    tdf3 = pd.read_csv(url.format('logic'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    tdf3 = tdf3.combine_first(tdf2)\n",
    "    t_df = tdf3.combine_first(tdf1)\n",
    "    if tstart is not None:\n",
    "        tstart = max(t_df.index.min(), pd.Timestamp(tstart))\n",
    "    else:\n",
    "        tstart = t_df.index.min()\n",
    "    t_df = t_df.groupby(common_date, axis=0).mean()\n",
    "    t_df = t_df.loc[(t_df.index >= tstart) & (t_df.index <= tend)]\n",
    "    new_dates = pd.date_range(start=tstart, end=tend, freq='D')\n",
    "    t_df = t_df.reindex(index=new_dates)\n",
    "    # Find the first non-NaN entry\n",
    "    tstart_min = t_df.loc[~t_df['t'].isnull()].index[0]\n",
    "    # Ensure the time series starts with a non-NaN value\n",
    "    t_df = t_df.loc[t_df.index >= tstart_min]\n",
    "    return interpolate_T(t_df)\n",
    "\n",
    "df2 = get_T(tstart=df1.index[0], tend=df1.index[-1])\n",
    "#df2 = get_T()\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df2.index, df2['t'], 'k--')\n",
    "plt.fill_between(df2.index, df2['t']-3*df2['t_err'],\n",
    "                  df2['t']+3*df2['t_err'], alpha=0.5)\n",
    "plt.plot(df2.index, df2['t_orig'], 'k+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_ll(df, dt=1):\n",
    "    dts = np.r_[0, np.cumsum(np.diff(df.index).astype(int)/(86400*1e9))]\n",
    "    dts = dts[:, np.newaxis]\n",
    "    ny = df['h'].values\n",
    "    ny = np.where(np.isnan(ny), None, ny)\n",
    "    \n",
    "    kf = KF(dim_x=1, dim_z=1)\n",
    "    kf.F = np.array([[1.]])\n",
    "    kf.H = np.array([[1.]])\n",
    "    if ny[1] is not None:\n",
    "        dT0 = ny[1] - ny[0]\n",
    "    else:\n",
    "        dT0 = 0.\n",
    "    kf.x = np.array([ny[0]])\n",
    "    kf.Q = 1e-2**2\n",
    "    kf.P = 0.03**2\n",
    "    kf.R = 0.02**2\n",
    "    means, covariances, _, _ =kf.batch_filter(ny[1:])\n",
    "    Ms, P, _, _ = kf.rts_smoother(means, covariances)\n",
    "    y_new = np.r_[ny[0], Ms[:,0]]\n",
    "    y_std = np.r_[0.03, np.sqrt(P[:, 0, 0])]\n",
    "    return pd.DataFrame({'h': y_new,\n",
    "                         'h_err': y_std,\n",
    "                         'h_orig': df['h'].values},\n",
    "                         index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll(tstart=None, tend=datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")):\n",
    "    # Get lake level\n",
    "    # The lake level data is stored with respect to the overflow level of\n",
    "    # the lake. Unfortunately, that level has changed over time so to get\n",
    "    # the absolute lake level altitude, data from different periods have to\n",
    "    # be corrected differently. Also, lake level data has been measured by\n",
    "    # different methods requiring several requests.\n",
    "    url = \"https://fits.geonet.org.nz/observation?siteID={}&typeID=z\"\n",
    "    names = ['h', 'h_err']\n",
    "    ldf = pd.read_csv(url.format('RU001'),\n",
    "                      index_col=0, names=names, skiprows=1,\n",
    "                      parse_dates=True)\n",
    "    ldf1 = pd.read_csv(url.format('RU001A'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    ll_df = ldf.combine_first(ldf1)\n",
    "    ll_df.loc[ll_df.index < '1997-01-01', 'h'] = 2530. + \\\n",
    "        ll_df.loc[ll_df.index < '1997-01-01', 'h']\n",
    "    ll_df.loc[(ll_df.index > '1997-01-01') & (ll_df.index < '2012-12-31'), 'h'] = 2529.5 + \\\n",
    "              (ll_df.loc[(ll_df.index > '1997-01-01') & (ll_df.index < '2012-12-31'), 'h'] - 1.3)\n",
    "    ll_df.loc[ll_df.index > '2016-01-01', 'h'] = 2529.35 + (ll_df.loc[ll_df.index > '2016-01-01', 'h'] - 2.0)\n",
    "    \n",
    "    if tstart is not None:\n",
    "        tstart = max(ll_df.index.min(), pd.Timestamp(tstart))\n",
    "    else:\n",
    "        tstart = ll_df.index.min()\n",
    "    ll_df = ll_df.groupby(common_date, axis=0).mean()\n",
    "    ll_df = ll_df.loc[(ll_df.index >= tstart) & (ll_df.index <= tend)]\n",
    "    new_dates = pd.date_range(start=tstart, end=tend, freq='D')\n",
    "    ll_df = ll_df.reindex(index=new_dates)\n",
    "    # Find the first non-NaN entry\n",
    "    tstart_min = ll_df.loc[~ll_df['h'].isnull()].index[0]\n",
    "    # Ensure the time series starts with a non-NaN value\n",
    "    ll_df = ll_df.loc[ll_df.index >= tstart_min]\n",
    "    return interpolate_ll(ll_df)\n",
    "\n",
    "df3 = get_ll(tstart=df1.index[0], tend=df1.index[-1])\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df3.index, df3['h'], 'k--')\n",
    "plt.fill_between(df3.index, df3['h']-3*df3['h_err'],\n",
    "                  df3['h']+3*df3['h_err'], alpha=0.5)\n",
    "plt.plot(df3.index, df3['h_orig'], 'k+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_obs(df1, df2, df3, nsamples=100):\n",
    "    \"\"\"\n",
    "    Compute absolute amount of Mg++, volume, lake aread,\n",
    "    water density and lake mass using Monte Carlo sampling\n",
    "    \"\"\"\n",
    "    rn1 = np.random.randn(df1['obs'].size, nsamples)\n",
    "    rn1 = rn1*np.tile(df1['obs_err'].values, (nsamples,1)).T + np.tile(df1['obs'].values, (nsamples,1)).T\n",
    "\n",
    "    rn2 = np.random.randn(df3['h'].size, nsamples)\n",
    "    rn2 = rn2*np.tile(df3['h_err'].values, (nsamples, 1)).T + np.tile(df3['h'].values, (nsamples,1)).T\n",
    "    a, vol = fullness(rn2)\n",
    "    X = rn1*vol*1.e-6\n",
    "    \n",
    "    p_mean = 1.003 - 0.00033 * df2['t'].values\n",
    "    p_std = 0.00033*df2['t_err'].values\n",
    "    \n",
    "    rn3 = np.random.randn(p_mean.size, nsamples)\n",
    "    rn3 = rn3*np.tile(p_std, (nsamples, 1)).T + np.tile(p_mean, (nsamples,1)).T\n",
    "    M = rn3*vol\n",
    "    return (X.mean(axis=1), X.std(axis=1), vol.mean(axis=1), vol.std(axis=1),\n",
    "            p_mean, p_std, M.mean(axis=1), M.std(axis=1),\n",
    "            a.mean(axis=1), a.std(axis=1))\n",
    "X, X_err, v, v_err, p, p_err, M, M_err, a, a_err = derived_obs(df1, df2, df3, nsamples=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'T': df2['t'],\n",
    "                   'T_err': df2['t_err'],\n",
    "                   'z': df3['h'],\n",
    "                   'z_err': df3['h_err'],\n",
    "                   'Mg': df1['obs'],\n",
    "                   'Mg_err': df1['obs_err'],\n",
    "                   'X': X,\n",
    "                   'X_err': X_err,\n",
    "                   'v': v,\n",
    "                   'v_err': v_err,\n",
    "                   'a': a,\n",
    "                   'a_err': a_err,\n",
    "                   'p': p,\n",
    "                   'p_err': p_err,\n",
    "                   'M': M,\n",
    "                   'M_err': M_err})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('measurements.h5','table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('measurements.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback function to compute the log-likelihood for nested sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        Callback.__init__(self)\n",
    "        \n",
    "    def set_data(self, y1, cov, time, datetime, dt, ws):\n",
    "        self.y1 = y1\n",
    "        self.ws = ws\n",
    "        # the precision matrix is the inverse of the \n",
    "        # covariance matrix\n",
    "        self.prec = np.linalg.inv(cov)\n",
    "        self.factor = -np.log(np.power(2.*np.pi, cov.shape[0]) + np.sqrt(np.linalg.det(cov)))\n",
    "        self.time = time\n",
    "        self.datetime = datetime\n",
    "        self.dt = dt\n",
    "        self.y_new = None\n",
    "\n",
    "    def run(self, vals):\n",
    "        try:\n",
    "            Q_in = vals[0]*0.0864\n",
    "            M_melt = vals[1]\n",
    "            Mout = vals[2]\n",
    "            H = vals[3]\n",
    "            T = vals[4]\n",
    "            M = vals[5]\n",
    "            X = vals[6]\n",
    "            a = vals[7]\n",
    "            v = vals[8]\n",
    "            y0 = np.array([T, M, X])\n",
    "            solar = esol(self.time, a, self.datetime)\n",
    "            y_new, steam, mevap = forward_model(y0, self.dt, a, v, Q_in, \n",
    "                                                M_melt, Mout, solar,\n",
    "                                                H, ws, method='euler')\n",
    "            self.y_new = y_new\n",
    "            lh = self.factor - 0.5*np.dot(y_new-self.y1,np.dot(self.prec,y_new-self.y1))\n",
    "        except:\n",
    "            raise SamplingException(\"Oh no!\")\n",
    "        return lh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = '2018-05-01'\n",
    "\n",
    "ndf = df.loc[df.index >= startdate]\n",
    "datet = ndf.index\n",
    "\n",
    "nsteps = ndf.shape[0] - 1\n",
    "dt = 1.\n",
    "\n",
    "nsamples = 10000\n",
    "nresample = 500\n",
    "nparams = 9\n",
    "\n",
    "qin = Uniform('qin', 0, 1000)\n",
    "m_in = Uniform('m_in', 0, 20)\n",
    "h = Constant('h', 6.)\n",
    "m_out = Uniform('m_out', 0, 20)\n",
    "ws = 4.5\n",
    "\n",
    "# return values\n",
    "qin_samples = np.zeros((nsteps, nresample))\n",
    "m_in_samples = np.zeros((nsteps, nresample))\n",
    "m_out_samples = np.zeros((nsteps, nresample))\n",
    "h_samples = np.zeros((nsteps, nresample))\n",
    "lh = np.zeros((nsteps, nresample))\n",
    "exp = np.zeros((nsteps, nparams))\n",
    "var = np.zeros((nsteps, nparams))\n",
    "model_data = np.zeros((nsteps, nresample, 3))\n",
    "steam = np.zeros((nsteps, nresample))\n",
    "mevap = np.zeros((nsteps, nresample))\n",
    "\n",
    "ns = NestedSampling()\n",
    "pycb = PyCallback()\n",
    "pycb.__disown__()\n",
    "ns.setCallback(pycb)\n",
    "\n",
    "with progressbar.ProgressBar(max_value=nsteps-1) as bar:\n",
    "    for i in range(nsteps):\n",
    "        # Take samples from the input\n",
    "        T = Normal('T', ndf['T'][i], ndf['T_err'][i])\n",
    "        M = Normal('M', ndf['M'][i], ndf['M_err'][i])\n",
    "        X = Normal('X', ndf['X'][i], ndf['X_err'][i])\n",
    "        v = Normal('v', ndf['v'][i], ndf['v_err'][i])\n",
    "        a = Normal('a', ndf['a'][i], ndf['a_err'][i])\n",
    "        T_sigma = ndf['T_err'][i+1]\n",
    "        M_sigma = ndf['M_err'][i+1]\n",
    "        X_sigma = ndf['X_err'][i+1]\n",
    "        cov = np.array([[T_sigma*T_sigma, 0., 0.],\n",
    "                        [0., M_sigma*M_sigma, 0.],\n",
    "                        [0., 0., X_sigma*X_sigma]])\n",
    "        T_next = ndf['T'][i+1]\n",
    "        M_next = ndf['M'][i+1]\n",
    "        X_next = ndf['X'][i+1]\n",
    "\n",
    "        y = np.array([T, M, X])\n",
    "        y_next = np.array([T_next, M_next, X_next])\n",
    "        pycb.set_data(y_next, cov, i*dt, datet, dt, ws)\n",
    "        rs = ns.explore(vars=[qin, m_in, m_out, h, T, M, X, a, v],\n",
    "                        initial_samples=100,\n",
    "                        maximum_steps=nsamples)\n",
    "        del T, M, X, v, a\n",
    "        smp = rs.resample_posterior(nresample)\n",
    "        exp[i,:] = rs.getexpt()\n",
    "        var[i,:] = rs.getvar()\n",
    "        for j,_s in enumerate(smp):\n",
    "            Q_in = _s._vars[0].get_value()\n",
    "            M_in = _s._vars[1].get_value()\n",
    "            M_out = _s._vars[2].get_value()\n",
    "            H = _s._vars[3].get_value()\n",
    "            T = _s._vars[4].get_value()\n",
    "            M = _s._vars[5].get_value()\n",
    "            X = _s._vars[6].get_value()\n",
    "            a = _s._vars[7].get_value()\n",
    "            v = _s._vars[8].get_value()\n",
    "            y = np.array([T, M, X])\n",
    "            solar = esol(i*dt, a, datet)\n",
    "            y_mod, st, me = forward_model(y, dt, a, v, Q_in*0.0864, \n",
    "                                          M_in, M_out, solar, H, ws)\n",
    "            steam[i, j] = st\n",
    "            mevap[i, j] = me\n",
    "            model_data[i, j, :] = y_mod\n",
    "            qin_samples[i, j] = Q_in\n",
    "            m_in_samples[i, j] = M_in\n",
    "            m_out_samples[i, j] = M_out\n",
    "            h_samples[i, j] = H\n",
    "            lh[i, j] = np.exp(_s._logL)\n",
    "        del smp\n",
    "        bar.update(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for input to clemb\n",
    "from clemb import Gauss, DataLoader\n",
    "class LakeData(DataLoader):\n",
    "    def get_data(self, start, end):\n",
    "        ndf = df.loc[start:end]\n",
    "        vd = {}\n",
    "        vd['t'] = Gauss(ndf['T'])\n",
    "        vd['m'] = Gauss(ndf['Mg'])\n",
    "        vd['c'] = Gauss(ndf['Mg'])\n",
    "        vd['h'] = Gauss(ndf['z'])\n",
    "        vd['dv'] = Gauss(pd.Series(np.ones(ndf.index.size), index=ndf.index))\n",
    "        return vd\n",
    "\n",
    "class WindData(DataLoader):\n",
    "    def get_data(self, start, end):\n",
    "        ndf = df.loc[start:end]\n",
    "        return Gauss(pd.Series(np.ones(ndf.index.size)*4.5, index=ndf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    c = clemb.Clemb(LakeData(), WindData(), start=startdate, end=tend)\n",
    "    c.use_drmg = True\n",
    "    rp = c.run([0,1])\n",
    "    rp.to_hdf('clemb_output.h5','table')\n",
    "else:\n",
    "    rp = pd.read_hdf('clemb_output.h5', 'table')\n",
    "pwr = rp.loc[0,'pwr']\n",
    "melt = rp.loc[0,'fmelt']\n",
    "orig_mevap = rp.loc[0, 'evfl']\n",
    "orig_steam = rp.loc[0, 'steam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask zeros\n",
    "from numpy.ma import masked_equal\n",
    "steam = masked_equal(steam, 0)\n",
    "mevap = masked_equal(mevap, 0)\n",
    "model_data = masked_equal(model_data, 0)\n",
    "qin_samples = masked_equal(qin_samples, 0)\n",
    "m_in_samples = masked_equal(m_in_samples, 0)\n",
    "m_out_samples = masked_equal(m_out_samples, 0)\n",
    "h_samples = masked_equal(h_samples, 0)\n",
    "lh = masked_equal(lh, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = mdates.DayLocator()  # every day\n",
    "months = mdates.MonthLocator()\n",
    "monthFmt = mdates.DateFormatter('%Y-%m-%d')\n",
    "dayFmt = mdates.DateFormatter('%d')\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.subplot.hspace'] = 0.5\n",
    "fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(14, 10))\n",
    "\n",
    "axs[0,0].plot(datet, np.ones(ndf.index.size)*4.5, ls='--')\n",
    "axs[0,0].set_title('Wind speed [m/s]')\n",
    "\n",
    "axs[0,1].plot(datet, ndf['X'], ls='--')\n",
    "\n",
    "axs[0,1].fill_between(datet, ndf['X']-3*ndf['X_err'],\n",
    "                      ndf['X']+3*ndf['X_err'], alpha=0.5)\n",
    "axs[0,1].plot(datet[:-1], model_data[:,:,2].mean(axis=1), 'k-')\n",
    "axs[0,1].plot(datet[:-1],\n",
    "              model_data[:,:,2].mean(axis=1)+3*model_data[:,:,2].std(axis=1),\n",
    "             'k--')\n",
    "axs[0,1].plot(datet[:-1],\n",
    "              model_data[:,:,2].mean(axis=1)-3*model_data[:,:,2].std(axis=1),\n",
    "              'k--')\n",
    "axs[0,1].set_title('Mg++ amount [kt]')\n",
    "\n",
    "axs[1,0].plot(datet, ndf['T'], ls='--')\n",
    "axs[1,0].fill_between(datet, ndf['T']-3*ndf['T_err'],\n",
    "                      ndf['T']+3*ndf['T_err'], alpha=0.5)\n",
    "axs[1,0].plot(datet[:-1], model_data[:,:,0].mean(axis=1), 'k-')\n",
    "axs[1,0].plot(datet[:-1],\n",
    "              model_data[:,:,0].mean(axis=1)+3*model_data[:,:,0].std(axis=1),\n",
    "             'k--')\n",
    "axs[1,0].plot(datet[:-1],\n",
    "              model_data[:,:,0].mean(axis=1)-3*model_data[:,:,0].std(axis=1),\n",
    "              'k--')\n",
    "axs[1,0].set_title('Lake temperature [$^{\\circ}C$]')\n",
    "\n",
    "axs[1,1].plot(datet, ndf['M'], ls='--')\n",
    "axs[1,1].fill_between(datet, ndf['M']-3*ndf['M_err'],\n",
    "                      ndf['M']+3*ndf['M_err'], alpha=0.5)\n",
    "axs[1,1].plot(datet[:-1], model_data[:,:,1].mean(axis=1), 'k-')\n",
    "axs[1,1].plot(datet[:-1],\n",
    "              model_data[:,:,1].mean(axis=1)+3*model_data[:,:,1].std(axis=1),\n",
    "             'k--')\n",
    "axs[1,1].plot(datet[:-1],\n",
    "              model_data[:,:,1].mean(axis=1)-3*model_data[:,:,1].std(axis=1),\n",
    "              'k--')\n",
    "axs[1,1].set_title('Lake mass [kt]')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[2,0].scatter([datet[k]]*nresample, qin_samples[k], s=2, c=lh[k],\n",
    "                     cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[2,0].plot(datet[:-1], exp[:,0], 'k')\n",
    "axs[2,0].plot(datet[:-1], exp[:,0] - 3*np.sqrt(var[:,0]), 'k--')\n",
    "axs[2,0].plot(datet[:-1], exp[:,0] + 3*np.sqrt(var[:,0]), 'k--')\n",
    "axs[2,0].plot(datet[:-1], pwr.values, 'b-')\n",
    "axs[2,0].set_title('Heat input rate [MW]')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[2,1].scatter([datet[k]]*nresample, h_samples[k], s=2, c=lh[k],\n",
    "                cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[2,1].plot(datet[:-1], exp[:,3], 'k')\n",
    "#axs[2,1].plot(np.arange(nsteps), exp[:,3] - 3*np.sqrt(var[:,3]), 'k--')\n",
    "#axs[2,1].plot(np.arange(nsteps), exp[:,3] + 3*np.sqrt(var[:,3]), 'k--')\n",
    "#axs[2,1].plot(np.arange(nsteps+1), np.ones(ndf.index.size)*6.0, ls='--')\n",
    "axs[2,1].set_title('Enthalpy [TJ/kt]')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[3,0].scatter([datet[k]]*nresample, m_in_samples[k], s=2, c=lh[k],\n",
    "                cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[3,0].plot(datet[:-1], exp[:,1], 'k')\n",
    "axs[3,0].plot(datet[:-1], exp[:,1] - 3*np.sqrt(var[:,1]), 'k--')\n",
    "axs[3,0].plot(datet[:-1], exp[:,1] + 3*np.sqrt(var[:,1]), 'k--')\n",
    "axs[3,0].plot(datet[:-1], melt.values, 'b-')\n",
    "axs[3,0].set_title('Inflow [kt/day]')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[3,1].scatter([datet[k]]*nresample, m_out_samples[k], s=2, c=lh[k],\n",
    "                cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[3,1].plot(datet[:-1], exp[:,2], 'k')\n",
    "axs[3,1].plot(datet[:-1], exp[:,2] - 3*np.sqrt(var[:,2]), 'k--')\n",
    "axs[3,1].plot(datet[:-1], exp[:,2] + 3*np.sqrt(var[:,2]), 'k--')\n",
    "axs[3,1].set_title('Outflow [kt/day]')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[4,0].scatter([datet[k]]*nresample, steam[k], s=2, c=lh[k],\n",
    "                     cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[4,0].plot(datet[:-1], orig_steam.values, 'b-')\n",
    "axs[4,0].set_title('Steam input [kt/day]')\n",
    "    \n",
    "for k in range(nsteps):\n",
    "    axs[4,1].scatter([datet[k]]*nresample, mevap[k], s=2, c=lh[k],\n",
    "                     cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[4,1].plot(datet[:-1], orig_mevap.values, 'b-')\n",
    "_ = axs[4,1].set_title('Evaporation mass loss [kt/day]')\n",
    "\n",
    "for row in range(5):\n",
    "    for col in range(2):\n",
    "        axs[row,col].xaxis.set_major_locator(months)\n",
    "        axs[row,col].xaxis.set_major_formatter(monthFmt)\n",
    "        axs[row,col].xaxis.set_minor_locator(days)\n",
    "fig.savefig('inference_result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "axs[0,0].hist(qin_samples[-1], bins=20)\n",
    "ylim = axs[0,0].get_ylim()\n",
    "axs[0,0].vlines(exp[-1,0], *ylim)\n",
    "axs[0,0].vlines(exp[-1,0] - np.sqrt(var[-1,0]), *ylim, linestyle='--')\n",
    "axs[0,0].vlines(exp[-1,0] + np.sqrt(var[-1,0]), *ylim, linestyle='--')\n",
    "axs[0,0].set_ylim(*ylim)\n",
    "axs[0,0].set_xlabel('Heat input rate [MW]')\n",
    "\n",
    "axs[0,1].hist(steam[-1], bins=20)\n",
    "ylim = axs[0,1].get_ylim()\n",
    "axs[0,1].vlines(steam[-1].mean(), *ylim)\n",
    "axs[0,1].vlines(steam[-1].mean() - steam[-1].std(), *ylim, linestyle='--')\n",
    "axs[0,1].vlines(steam[-1].mean() + steam[-1].std(), *ylim, linestyle='--')\n",
    "axs[0,1].set_ylim(*ylim)\n",
    "axs[0,1].set_xlabel('Steam input rate [kt/day]')\n",
    "\n",
    "axs[1,0].hist(m_in_samples[-1], bins=20)\n",
    "axs[1,0].vlines(exp[-1,1], *ylim)\n",
    "axs[1,0].vlines(exp[-1,1] - np.sqrt(var[-1,1]), *ylim, linestyle='--')\n",
    "axs[1,0].vlines(exp[-1,1] + np.sqrt(var[-1,1]), *ylim, linestyle='--')\n",
    "axs[1,0].set_ylim(*ylim)\n",
    "axs[1,0].set_xlabel('Water inflow rate [kt/day]')\n",
    "\n",
    "axs[1,1].hist(m_out_samples[-1], bins=20)\n",
    "axs[1,1].vlines(exp[-1,2], *ylim)\n",
    "axs[1,1].vlines(exp[-1,2] - np.sqrt(var[-1,2]), *ylim, linestyle='--')\n",
    "axs[1,1].vlines(exp[-1,2] + np.sqrt(var[-1,2]), *ylim, linestyle='--')\n",
    "axs[1,1].set_ylim(*ylim)\n",
    "_ = axs[1,1].set_xlabel('Water outflow rate [kt/day]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullness_inv(v):\n",
    "    \"\"\"\n",
    "    Compute lake level from lake volume.\n",
    "    \"\"\"\n",
    "    def f(h, v):\n",
    "        return (4.747475*np.power(h, 3)-34533.8*np.power(h, 2) + 83773360.*h-67772125000.)/1000. - v\n",
    "    return brentq(f, 2400, 2600, args=v)\n",
    "\n",
    "def f_x(x, dt, dQin, datetime, time, H):\n",
    "    \"\"\"\n",
    "    Forward model lake temperature\n",
    "    \"\"\"\n",
    "    Q_in = x[0]\n",
    "    M_in = x[1]\n",
    "    M_out = x[2]\n",
    "    T = x[3]\n",
    "    M = x[4]\n",
    "    X = x[5]\n",
    "    a = x[6]\n",
    "    v = x[7]\n",
    "    y = np.array([T, M, X])\n",
    "    solar = esol(i * dt, a, datet)\n",
    "    y_next, st, me = forward_model(y, dt, a, v, Q_in * 0.0864, \n",
    "                                   M_in, M_out, solar, H, ws)\n",
    "    p0 = 1.003 - 0.00033 * T\n",
    "    p1 = 1.003 - 0.00033 * y_next[0]\n",
    "    v0 = M/p0\n",
    "    v1 = y_next[1]/p1\n",
    "    h0 = fullness_inv(v0)\n",
    "    h1 = fullness_inv(v1)\n",
    "    dh = h1 - h0\n",
    "    a_next = (v1 - v0)*1e3/(h1 - h0)\n",
    "    q_in_next = Q_in + dQin * dt\n",
    "    return np.array([q_in_next, M_in, M_out, \n",
    "                     y_next[0], y_next[1], y_next[2],\n",
    "                     a_next, v1])\n",
    "\n",
    "def h_x(x):\n",
    "    \"\"\"\n",
    "    Measurement function\n",
    "    \"\"\"\n",
    "    return [x[0]]\n",
    "\n",
    "points = MerweScaledSigmaPoints(n=8, alpha=.1, beta=2., kappa=0.)\n",
    "kf = UKF(dim_x=8, dim_z=1, dt=dt, fx=f_x, hx=h_x, points=points)\n",
    "x = np.r_[exp[-1,0:3], exp[-1,4:]]\n",
    "P = np.eye(8)*np.r_[var[-1,0:3],var[-1,4:]]\n",
    "kf.x = x\n",
    "kf.Q = np.eye(8)*1e-7\n",
    "kf.P = P\n",
    "kf.R = 50.**2\n",
    "nperiods=10\n",
    "datetime_new = pd.date_range(start=datet[-1], periods=nperiods, freq='D')\n",
    "dQin = exp[-1,0] - exp[-2,0]\n",
    "means = np.zeros((nperiods, 8))\n",
    "covariances = np.zeros((nperiods, 8, 8))\n",
    "for i in range(nperiods):\n",
    "    kf.predict(dQin=dQin, time=i*dt,\n",
    "               datetime=datetime_new,\n",
    "               H=exp[-1,3])\n",
    "    means[i,:] = kf.x\n",
    "    covariances[i, :, :] = kf.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nT = get_T(tstart=tstart)\n",
    "nT_crop = nT.loc[nT.index >= startdate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_pred = np.r_[ndf['T'][:], means[:,3]]\n",
    "dates = np.r_[datet[:], datetime_new+1]\n",
    "nvar = np.r_[ndf['T_err'][:], np.sqrt(covariances[:, 3, 3])]\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(dates, T_pred)\n",
    "plt.fill_between(dates, T_pred -2*nvar, T_pred + 2*nvar, alpha=0.5)\n",
    "plt.plot(nT_crop.index, nT_crop['t_orig'], 'k+')\n",
    "_ = plt.ylabel(r'Temperature [$^\\circ$C]')\n",
    "plt.savefig('RCL_T_prediction.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Conda",
   "language": "python",
   "name": "python3_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
