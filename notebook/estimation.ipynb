{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sampling import NestedSampling, Uniform, Callback, Normal, SamplingException\n",
    "\n",
    "from filterpy.kalman import UnscentedKalmanFilter as UKF\n",
    "from filterpy.kalman import KalmanFilter as KF\n",
    "from filterpy.kalman import MerweScaledSigmaPoints\n",
    "from filterpy.common import Q_continuous_white_noise\n",
    "\n",
    "import progressbar\n",
    "\n",
    "from clemb.forward_model import forward_model, fullness, esol\n",
    "import clemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---------------------------\n",
    "1. [Data preparation](#Data preparation)\n",
    "2. [Inversion](#Inversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_date(date):\n",
    "    \"\"\"\n",
    "    Function used for pandas group-by.\n",
    "    If there are several measurements in \n",
    "    one day, take the mean.\n",
    "    \"\"\"\n",
    "    ndt = pd.Timestamp(year=date.year,\n",
    "                       month=date.month,\n",
    "                       day=date.day)\n",
    "    return ndt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_mg(df, dt=1):\n",
    "    \"\"\"\n",
    "    Inter- and extrapolate Mg++ measurements using a\n",
    "    non-linear Kalman filter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def f_x(x, dt):\n",
    "        \"\"\"\n",
    "        Forward model exponential decay\n",
    "        \"\"\"\n",
    "        _k = x[1]/1e3\n",
    "        _dt = dt\n",
    "        _y = x[0]\n",
    "        if isinstance(dt, np.ndarray):\n",
    "            _dt = dt[0]\n",
    "        # 4th order Runge-Kutta\n",
    "        k0 = -_k * _y * _dt\n",
    "        k1 = -_k * (_y + 0.5 * k0) * _dt\n",
    "        k2 = -_k * (_y + 0.5 * k1) * _dt\n",
    "        k3 = -_k * (_y + k2) * _dt\n",
    "        _y_next = _y + 1./6.*(k0 + 2 * k1 + 2 * k2 + k3)\n",
    "        return np.array([_y_next, x[1]])\n",
    "\n",
    "    def h_x(x):\n",
    "        \"\"\"\n",
    "        Measurement function\n",
    "        \"\"\"\n",
    "        return [x[0]]\n",
    "    \n",
    "    dts = np.r_[0, np.cumsum(np.diff(df.index).astype(int)/(86400*1e9))]\n",
    "    dts = dts[:, np.newaxis]\n",
    "    ny = df['obs'].values\n",
    "    ny = np.where(np.isnan(ny), None, ny)\n",
    "    \n",
    "    points = MerweScaledSigmaPoints(n=2, alpha=.01, beta=2., kappa=1.)\n",
    "    kf = UKF(dim_x=2, dim_z=1, dt=dt, fx=f_x, hx=h_x, points=points)\n",
    "    kf.x = np.array([ny[0], .6])\n",
    "    kf.Q = Q_continuous_white_noise(2, dt=dt, spectral_density=1e-5)\n",
    "    kf.P = np.diag([100.**2, 3.**2])\n",
    "    kf.R = 50.**2\n",
    "    npoints = dts.size\n",
    "    means = np.zeros((npoints-1, 2))\n",
    "    covariances = np.zeros((npoints-1, 2, 2))\n",
    "    for i, z_n in enumerate(ny[1:]):\n",
    "        kf.predict()\n",
    "        kf.update(z_n)\n",
    "        means[i,:] = kf.x\n",
    "        covariances[i, :, :] = kf.P\n",
    "    Ms, P, K = kf.rts_smoother(means, covariances)\n",
    "    y_new = np.r_[ny[0], Ms[:,0]]\n",
    "    y_std = np.r_[50, np.sqrt(P[:, 0, 0])]\n",
    "    return pd.DataFrame({'obs': y_new,\n",
    "                         'obs_err': y_std,\n",
    "                         'orig': df['obs'].values},\n",
    "                        index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mg_data(tstart=None, tend=datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")):\n",
    "    # Get Mg++ concentration\n",
    "    url = \"https://fits.geonet.org.nz/observation?siteID=RU003&typeID=Mg-w\"\n",
    "    names = ['obs', 'obs_err']\n",
    "    mg_df = pd.read_csv(url, index_col=0, names=names, skiprows=1,\n",
    "                        parse_dates=True)\n",
    "    if tstart is not None:\n",
    "        tstart = max(mg_df.index.min(), pd.Timestamp(tstart))\n",
    "    else:\n",
    "        tstart = mg_df.index.min()\n",
    "    mg_df = mg_df.loc[(mg_df.index >= tstart) & (mg_df.index <= tend)]\n",
    "\n",
    "    mg_df = mg_df.groupby(common_date, axis=0).mean()\n",
    "    new_dates = pd.date_range(start=tstart, end=tend, freq='D')\n",
    "    mg_df = mg_df.reindex(index=new_dates)\n",
    "    # Find the first non-NaN entry\n",
    "    tstart_min = mg_df.loc[~mg_df['obs'].isnull()].index[0]\n",
    "    # Ensure the time series starts with a non-NaN value\n",
    "    mg_df = mg_df.loc[mg_df.index >= tstart_min]\n",
    "    return interpolate_mg(mg_df)\n",
    "\n",
    "tstart = '2009-09-25'\n",
    "df1 = get_mg_data(tstart=tstart)\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df1.index, df1['obs'], 'k--')\n",
    "plt.fill_between(df1.index, df1['obs']-3*df1['obs_err'],\n",
    "                  df1['obs']+3*df1['obs_err'], alpha=0.5)\n",
    "plt.plot(df1.index, df1['orig'], 'k+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_T(df, dt=1):\n",
    "    dts = np.r_[0, np.cumsum(np.diff(df.index).astype(int)/(86400*1e9))]\n",
    "    dts = dts[:, np.newaxis]\n",
    "    ny = df['t'].values\n",
    "    ny = np.where(np.isnan(ny), None, ny)\n",
    "    \n",
    "    kf = KF(dim_x=2, dim_z=1)\n",
    "    kf.F = np.array([[1, 1], [0, 1]])\n",
    "    kf.H = np.array([[1., 0]])\n",
    "    if ny[1] is not None:\n",
    "        dT0 = ny[1] - ny[0]\n",
    "    else:\n",
    "        dT0 = 0.\n",
    "    kf.x = np.array([ny[0], dT0])\n",
    "    kf.Q = Q_continuous_white_noise(2, dt=dt, spectral_density=3e-2)\n",
    "    kf.P *= 1e-5**2\n",
    "    kf.R = .5**2\n",
    "    means, covariances, _, _ =kf.batch_filter(ny[1:])\n",
    "    Ms, P, _, _ = kf.rts_smoother(means, covariances)\n",
    "    y_new = np.r_[ny[0], Ms[:,0]]\n",
    "    y_std = np.r_[.3, np.sqrt(P[:, 0, 0])]\n",
    "    return pd.DataFrame({'t': y_new,\n",
    "                         't_err': y_std,\n",
    "                         't_orig': df['t'].values},\n",
    "                         index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_T(tstart=None, tend=datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")):        \n",
    "    # Get temperature\n",
    "    # Temperature has been recorded by 3 different sensors so 3 individual\n",
    "    # requests have to be made\n",
    "    url = \"https://fits.geonet.org.nz/observation?siteID=RU001&typeID=t&methodID={}\"\n",
    "    names = ['t', 't_err']\n",
    "    tdf1 = pd.read_csv(url.format('therm'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    tdf2 = pd.read_csv(url.format('thermcoup'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    tdf3 = pd.read_csv(url.format('logic'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    tdf3 = tdf3.combine_first(tdf2)\n",
    "    t_df = tdf3.combine_first(tdf1)\n",
    "    if tstart is not None:\n",
    "        tstart = max(t_df.index.min(), pd.Timestamp(tstart))\n",
    "    else:\n",
    "        tstart = t_df.index.min()\n",
    "    t_df = t_df.loc[(t_df.index >= tstart) & (t_df.index <= tend)]\n",
    "    t_df = t_df.groupby(common_date, axis=0).mean()\n",
    "    new_dates = pd.date_range(start=tstart, end=tend, freq='D')\n",
    "    t_df = t_df.reindex(index=new_dates)\n",
    "    # Find the first non-NaN entry\n",
    "    tstart_min = t_df.loc[~t_df['t'].isnull()].index[0]\n",
    "    # Ensure the time series starts with a non-NaN value\n",
    "    t_df = t_df.loc[t_df.index >= tstart_min]\n",
    "    return interpolate_T(t_df)\n",
    "\n",
    "df2 = get_T(tstart=df1.index[0], tend=df1.index[-1])\n",
    "#df2 = get_T()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df2.index, df2['t'], 'k--')\n",
    "plt.fill_between(df2.index, df2['t']-3*df2['t_err'],\n",
    "                  df2['t']+3*df2['t_err'], alpha=0.5)\n",
    "plt.plot(df2.index, df2['t_orig'], 'k+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_ll(df, dt=1):\n",
    "    dts = np.r_[0, np.cumsum(np.diff(df.index).astype(int)/(86400*1e9))]\n",
    "    dts = dts[:, np.newaxis]\n",
    "    ny = df['h'].values\n",
    "    ny = np.where(np.isnan(ny), None, ny)\n",
    "    \n",
    "    kf = KF(dim_x=1, dim_z=1)\n",
    "    kf.F = np.array([[1.]])\n",
    "    kf.H = np.array([[1.]])\n",
    "    if ny[1] is not None:\n",
    "        dT0 = ny[1] - ny[0]\n",
    "    else:\n",
    "        dT0 = 0.\n",
    "    kf.x = np.array([ny[0]])\n",
    "    kf.Q = 1e-2**2\n",
    "    kf.P = 0.03**2\n",
    "    kf.R = 0.02**2\n",
    "    means, covariances, _, _ =kf.batch_filter(ny[1:])\n",
    "    Ms, P, _, _ = kf.rts_smoother(means, covariances)\n",
    "    y_new = np.r_[ny[0], Ms[:,0]]\n",
    "    y_std = np.r_[0.03, np.sqrt(P[:, 0, 0])]\n",
    "    return pd.DataFrame({'h': y_new,\n",
    "                         'h_err': y_std,\n",
    "                         'h_orig': df['h'].values},\n",
    "                         index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll(tstart=None, tend=datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")):\n",
    "    # Get lake level\n",
    "    # The lake level data is stored with respect to the overflow level of\n",
    "    # the lake. Unfortunately, that level has changed over time so to get\n",
    "    # the absolute lake level altitude, data from different periods have to\n",
    "    # be corrected differently. Also, lake level data has been measured by\n",
    "    # different methods requiring several requests.\n",
    "    url = \"https://fits.geonet.org.nz/observation?siteID={}&typeID=z\"\n",
    "    names = ['h', 'h_err']\n",
    "    ldf = pd.read_csv(url.format('RU001'),\n",
    "                      index_col=0, names=names, skiprows=1,\n",
    "                      parse_dates=True)\n",
    "    ldf1 = pd.read_csv(url.format('RU001A'),\n",
    "                       index_col=0, names=names, skiprows=1,\n",
    "                       parse_dates=True)\n",
    "    ll_df = ldf.combine_first(ldf1)\n",
    "    ll_df.loc[ll_df.index < '1997-01-01', 'h'] = 2530. + \\\n",
    "        ll_df.loc[ll_df.index < '1997-01-01', 'h']\n",
    "    ll_df.loc[(ll_df.index > '1997-01-01') & (ll_df.index < '2012-12-31'), 'h'] = 2529.5 + \\\n",
    "              (ll_df.loc[(ll_df.index > '1997-01-01') & (ll_df.index < '2012-12-31'), 'h'] - 1.3)\n",
    "    ll_df.loc[ll_df.index > '2016-01-01', 'h'] = 2529.35 + (ll_df.loc[ll_df.index > '2016-01-01', 'h'] - 2.0)\n",
    "    \n",
    "    if tstart is not None:\n",
    "        tstart = max(ll_df.index.min(), pd.Timestamp(tstart))\n",
    "    else:\n",
    "        tstart = ll_df.index.min()\n",
    "    ll_df = ll_df.loc[(ll_df.index >= tstart) & (ll_df.index <= tend)]\n",
    "    ll_df = ll_df.groupby(common_date, axis=0).mean()\n",
    "    new_dates = pd.date_range(start=tstart, end=tend, freq='D')\n",
    "    ll_df = ll_df.reindex(index=new_dates)\n",
    "    # Find the first non-NaN entry\n",
    "    tstart_min = ll_df.loc[~ll_df['h'].isnull()].index[0]\n",
    "    # Ensure the time series starts with a non-NaN value\n",
    "    ll_df = ll_df.loc[ll_df.index >= tstart_min]\n",
    "    return interpolate_ll(ll_df)\n",
    "\n",
    "df3 = get_ll(tstart=df1.index[0], tend=df1.index[-1])\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df3.index, df3['h'], 'k--')\n",
    "plt.fill_between(df3.index, df3['h']-3*df3['h_err'],\n",
    "                  df3['h']+3*df3['h_err'], alpha=0.5)\n",
    "plt.plot(df3.index, df3['h_orig'], 'k+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_obs(df1, df2, df3, nsamples=100):\n",
    "    \"\"\"\n",
    "    Compute absolute amount of Mg++, volume, lake aread,\n",
    "    water density and lake mass using Monte Carlo sampling\n",
    "    \"\"\"\n",
    "    rn1 = np.random.randn(df1['obs'].size, nsamples)\n",
    "    rn1 = rn1*np.tile(df1['obs_err'].values, (nsamples,1)).T + np.tile(df1['obs'].values, (nsamples,1)).T\n",
    "\n",
    "    rn2 = np.random.randn(df3['h'].size, nsamples)\n",
    "    rn2 = rn2*np.tile(df3['h_err'].values, (nsamples, 1)).T + np.tile(df3['h'].values, (nsamples,1)).T\n",
    "    a, vol = fullness(rn2)\n",
    "    X = rn1*vol\n",
    "    \n",
    "    p_mean = 1.003 - 0.00033 * df2['t'].values\n",
    "    p_std = 0.00033*df2['t_err'].values\n",
    "    \n",
    "    rn3 = np.random.randn(p_mean.size, nsamples)\n",
    "    rn3 = rn3*np.tile(p_std, (nsamples, 1)).T + np.tile(p_mean, (nsamples,1)).T\n",
    "    M = rn3*vol\n",
    "    return (X.mean(axis=1), X.std(axis=1), vol.mean(axis=1), vol.std(axis=1),\n",
    "            p_mean, p_std, M.mean(axis=1), M.std(axis=1),\n",
    "            a.mean(axis=1), a.std(axis=1))\n",
    "X, X_err, v, v_err, p, p_err, M, M_err, a, a_err = derived_obs(df1, df2, df3, nsamples=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'T': df2['t'],\n",
    "                   'T_err': df2['t_err'],\n",
    "                   'z': df3['h'],\n",
    "                   'z_err': df3['h_err'],\n",
    "                   'Mg': df1['obs'],\n",
    "                   'Mg_err': df1['obs_err'],\n",
    "                   'X': X,\n",
    "                   'X_err': X_err,\n",
    "                   'v': v,\n",
    "                   'v_err': v_err,\n",
    "                   'a': a,\n",
    "                   'a_err': a_err,\n",
    "                   'p': p,\n",
    "                   'p_err': p_err,\n",
    "                   'M': M,\n",
    "                   'M_err': M_err})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('measurements.h5','table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('measurements.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback function to compute the log-likelihood for nested sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        Callback.__init__(self)\n",
    "        \n",
    "    def set_data(self, y0, y1, cov, vol, a, solar, dt, ws):\n",
    "        self.y0 = y0\n",
    "        self.y1 = y1\n",
    "        self.ws = ws\n",
    "        self.vol = vol\n",
    "        self.a = a\n",
    "        self.solar = solar\n",
    "        self.dt = dt\n",
    "        self.cov = cov\n",
    "        # the precision matrix is the inverse of the \n",
    "        # covariance matrix\n",
    "        self.prec = np.linalg.inv(cov)\n",
    "        self.factor = -np.log(np.power(2.*np.pi, self.cov.shape[0]) + np.sqrt(np.linalg.det(self.cov)))\n",
    "        self.y_new = None\n",
    "\n",
    "    def run(self, vals):\n",
    "        try:\n",
    "            Q_in = vals[0]*0.0864\n",
    "            M_melt = vals[1]\n",
    "            Mout = vals[2]*86400.*1.e-6 # convert l/s into 1e3 m^3/day\n",
    "            H = 6.\n",
    "\n",
    "            y_new, steam, mevap = forward_model(self.y0, dt, self.a, self.vol, Q_in, \n",
    "                                     M_melt, Mout, self.solar, H, self.ws)\n",
    "            self.y_new = y_new\n",
    "            lh = self.factor - 0.5*np.dot(y_new-self.y1,np.dot(self.prec,y_new-self.y1))\n",
    "        except:\n",
    "            raise SamplingException(\"Oh no!\")\n",
    "        return lh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = '2018-02-01'\n",
    "\n",
    "ndf = df.loc[df.index >= startdate]\n",
    "datetime = ndf.index\n",
    "\n",
    "nsteps = ndf.shape[0] - 1\n",
    "dt = 1.\n",
    "\n",
    "nsamples = 4000\n",
    "nresample = 100\n",
    "nparams = 3\n",
    "\n",
    "qin = Uniform('qin', 0, 800)\n",
    "m_in = Uniform('m_in', 0, 20)\n",
    "#h = Normal('h', 6., .5)\n",
    "m_out = Uniform('m_out', 30, 80)\n",
    "ws = 4.5\n",
    "\n",
    "# return values\n",
    "qin_samples = np.zeros((nsteps, nresample))\n",
    "m_in_samples = np.zeros((nsteps, nresample))\n",
    "m_out_samples = np.zeros((nsteps, nresample))\n",
    "h_samples = np.zeros((nsteps, nresample))\n",
    "lh = np.zeros((nsteps, nresample))\n",
    "exp = np.zeros((nsteps, nparams))\n",
    "var = np.zeros((nsteps, nparams))\n",
    "model_data = np.zeros((nsteps, nresample, 3))\n",
    "steam = np.zeros((nsteps, nresample))\n",
    "mevap = np.zeros((nsteps, nresample))\n",
    "\n",
    "ns = NestedSampling()\n",
    "pycb = PyCallback()\n",
    "pycb.__disown__()\n",
    "ns.setCallback(pycb)\n",
    "\n",
    "with progressbar.ProgressBar(max_value=nsteps-1) as bar:\n",
    "    for i in range(nsteps):\n",
    "        a = ndf['a'][i]\n",
    "        vol = ndf['v'][i]\n",
    "        solar = esol(i*dt, a, datetime)\n",
    "        # Take samples from the input\n",
    "        T = ndf['T'][i]\n",
    "        T_sigma = ndf['T_err'][i+1]\n",
    "        M = ndf['M'][i]\n",
    "        M_sigma = ndf['M_err'][i+1]\n",
    "        X = ndf['X'][i]\n",
    "        X_sigma = ndf['X_err'][i+1]\n",
    "        cov = np.array([[T_sigma, 0., 0.],[0., M_sigma, 0.], [0., 0., X_sigma]])\n",
    "        T_next = ndf['T'][i+1]\n",
    "        M_next = ndf['M'][i+1]\n",
    "        X_next = ndf['X'][i+1]\n",
    "\n",
    "        y = np.array([T, M, X])\n",
    "        y_next = np.array([T_next, M_next, X_next])\n",
    "        pycb.set_data(y, y_next, cov, vol, a, solar, dt, ws)\n",
    "        rs = ns.explore(vars=[qin, m_in, m_out], initial_samples=100,\n",
    "                        maximum_steps=nsamples)\n",
    "        smp = rs.resample_posterior(nresample)\n",
    "        exp[i,:] = rs.getexpt()\n",
    "        var[i,:] = rs.getvar()\n",
    "        for j,_s in enumerate(smp):\n",
    "            Q_in = _s._vars[0].get_value()\n",
    "            M_in = _s._vars[1].get_value()\n",
    "            M_out = _s._vars[2].get_value()\n",
    "            #H = _s._vars[3].get_value()\n",
    "            y_mod, st, me = forward_model(y, dt, a, vol, Q_in*0.0864, \n",
    "                                          M_in, M_out, solar, 6., ws)\n",
    "            steam[i, j] = st\n",
    "            mevap[i, j] = me\n",
    "            model_data[i, j, :] = y_mod\n",
    "            qin_samples[i, j] = Q_in\n",
    "            m_in_samples[i, j] = M_in\n",
    "            m_out_samples[i, j] = M_out\n",
    "            #h_samples[i, j] = H\n",
    "            lh[i, j] = np.exp(_s._logL)\n",
    "        del smp\n",
    "        bar.update(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    c = clemb.Clemb(clemb.LakeDataFITS(), clemb.WindDataCSV(), start=startdate, end=ndf.index[-1])\n",
    "    rp = c.run([0,1])\n",
    "    rp.to_hdf('clemb_output.h5','table')\n",
    "else:\n",
    "    rp = pd.read_hdf('clemb_output.h5', 'table')\n",
    "pwr = rp.loc[0,'pwr']\n",
    "melt = rp.loc[0,'fmelt']\n",
    "orig_mevap = rp.loc[0, 'evfl']\n",
    "orig_steam = rp.loc[0, 'steam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.subplot.hspace'] = 0.5\n",
    "fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(10, 8))\n",
    "\n",
    "axs[0,0].plot(np.arange(nsteps+1), np.ones(ndf.index.size)*4.5, ls='--')\n",
    "axs[0,0].set_title('Wind speed [m/s]')\n",
    "\n",
    "axs[0,1].plot(np.arange(nsteps+1), ndf['X'], ls='--')\n",
    "\n",
    "axs[0,1].fill_between(np.arange(nsteps+1), ndf['X']-3*ndf['X_err'],\n",
    "                      ndf['X']+3*ndf['X_err'], alpha=0.5)\n",
    "axs[0,1].plot(np.arange(nsteps), model_data[:,:,2].mean(axis=1))\n",
    "axs[0,1].set_title('Mg++ amount')\n",
    "\n",
    "axs[1,0].plot(np.arange(nsteps+1), ndf['T'], ls='--')\n",
    "axs[1,0].fill_between(np.arange(nsteps+1), ndf['T']-3*ndf['T_err'],\n",
    "                      ndf['T']+3*ndf['T_err'], alpha=0.5)\n",
    "#for k in range(nsteps):\n",
    "#    axs[1,0].scatter(np.ones(nresample)*k, model_data[k,:,0], s=2, c=lh[k],\n",
    "#                     cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[1,0].plot(np.arange(nsteps), model_data[:,:,0].mean(axis=1), 'k-')\n",
    "axs[1,0].plot(np.arange(nsteps),\n",
    "              model_data[:,:,0].mean(axis=1)+model_data[:,:,0].std(axis=1),\n",
    "             'k--')\n",
    "axs[1,0].plot(np.arange(nsteps),\n",
    "              model_data[:,:,0].mean(axis=1)-model_data[:,:,0].std(axis=1),\n",
    "              'k--')\n",
    "axs[1,0].set_title('Lake temperature')\n",
    "\n",
    "axs[1,1].plot(np.arange(nsteps+1), ndf['M'], ls='--')\n",
    "axs[1,1].fill_between(np.arange(nsteps+1), ndf['M']-3*ndf['M_err'],\n",
    "                      ndf['M']+3*ndf['M_err'], alpha=0.5)\n",
    "axs[1,1].plot(np.arange(nsteps), model_data[:,:,1].mean(axis=1))\n",
    "axs[1,1].set_title('Lake mass')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[2,0].scatter(np.ones(nresample)*k, qin_samples[k], s=2, c=lh[k],\n",
    "                     cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[2,0].plot(np.arange(nsteps), exp[:,0], 'k')\n",
    "axs[2,0].plot(np.arange(nsteps), exp[:,0] - 3*np.sqrt(var[:,0]), 'k--')\n",
    "axs[2,0].plot(np.arange(nsteps), exp[:,0] + 3*np.sqrt(var[:,0]), 'k--')\n",
    "axs[2,0].plot(np.arange(nsteps), pwr.values, 'b-')\n",
    "axs[2,0].set_title('Heat input rate')\n",
    "\n",
    "if False:\n",
    "    for k in range(nsteps):\n",
    "        axs[2,1].scatter(np.ones(nresample)*k, h_samples[k], s=2, c=lh[k],\n",
    "                    cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "    axs[2,1].plot(np.arange(nsteps), exp[:,3], 'k')\n",
    "    axs[2,1].plot(np.arange(nsteps), exp[:,3] - 3*np.sqrt(var[:,3]), 'k--')\n",
    "    axs[2,1].plot(np.arange(nsteps), exp[:,3] + 3*np.sqrt(var[:,3]), 'k--')\n",
    "axs[2,1].plot(np.arange(nsteps+1), np.ones(ndf.index.size)*6.0, ls='--')\n",
    "axs[2,1].set_title('Enthalpy')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[3,0].scatter(np.ones(nresample)*k, m_in_samples[k], s=2, c=lh[k],\n",
    "                cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[3,0].plot(np.arange(nsteps), exp[:,1], 'k')\n",
    "axs[3,0].plot(np.arange(nsteps), exp[:,1] - 3*np.sqrt(var[:,1]), 'k--')\n",
    "axs[3,0].plot(np.arange(nsteps), exp[:,1] + 3*np.sqrt(var[:,1]), 'k--')\n",
    "axs[3,0].plot(np.arange(nsteps), melt.values, 'b-')\n",
    "axs[3,0].set_title('Inflow')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[3,1].scatter(np.ones(nresample)*k, m_out_samples[k], s=2, c=lh[k],\n",
    "                cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "#axs[3,1].plot(np.arange(nsteps), exp[:,3], 'k')\n",
    "#axs[3,1].plot(np.arange(nsteps), exp[:,3] - 3*np.sqrt(var[:,3]), 'k--')\n",
    "#axs[3,1].plot(np.arange(nsteps), exp[:,3] + 3*np.sqrt(var[:,3]), 'k--')\n",
    "axs[3,1].set_title('Outflow')\n",
    "\n",
    "for k in range(nsteps):\n",
    "    axs[4,0].scatter(np.ones(nresample)*k, steam[k], s=2, c=lh[k],\n",
    "                     cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[4,0].plot(np.arange(nsteps), orig_steam.values, 'b-')\n",
    "axs[4,0].set_title('Steam input')\n",
    "    \n",
    "for k in range(nsteps):\n",
    "    axs[4,1].scatter(np.ones(nresample)*k, mevap[k], s=2, c=lh[k],\n",
    "                     cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.3)\n",
    "axs[4,1].plot(np.arange(nsteps), orig_mevap.values, 'b-')\n",
    "axs[4,1].set_title('Evaporation mass loss')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Conda",
   "language": "python",
   "name": "python3_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
